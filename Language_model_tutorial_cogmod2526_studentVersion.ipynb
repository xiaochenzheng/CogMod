{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaochenzheng/CogMod/blob/main/Language_model_tutorial_cogmod2526_studentVersion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcu6YpLym-_T"
      },
      "source": [
        "# Tutorial: Using Language Models to Probe Cognitive and Neural Representations\n",
        "_For the course Cognitive Modelling (2025-2026, DataScience & AI)_\n",
        "\n",
        "_Last updated: 26 October 2025_\n",
        "\n",
        "\n",
        "\n",
        "In this tutorial, we will explore how linguistic meaning is represented both in **computational models** and in the **human brain**.  \n",
        "\n",
        "1. **Part 1 ‚Äì Static Word Embeddings**  \n",
        "   We begin with classic embedding models such as **GloVe**, visualizing how words like *king*, *queen*, *man*, and *woman* are organized in a continuous semantic space.\n",
        "2. **Part 2 ‚Äì Contextualized Word Embeddings**  \n",
        "   We then move to **BERT**, a transformer-based model that represents words differently depending on their context. You will compare how the meaning of the word *mouse* changes across sentences and compute representational similarity between embeddings.\n",
        "\n",
        "3. **Part 3 ‚Äì Linking Models to the Brain**  \n",
        "   Finally, you will relate language model output to **fMRI data** recorded while participants read a story. Using **Representational Similarity Analysis (RSA)**, you will test whether deeper, more contextualized model layers better align with brain activity in language areas compared to early layers.\n",
        "\n",
        "Together, these exercises will show how language models can serve as computational tools for studying human cognitive and neural representations.\n",
        "\n",
        "\n",
        "Dr. Xiaochen Zheng\n",
        "\n",
        "With the help of chatGPT\n",
        "\n",
        "\n",
        "Based on: Advanced Neural and Cognitive Modelling (https://clclab.github.io/ANCM/intro.html) by J.Zuidema & M. de Heer Kloots\n",
        "\n",
        "\n",
        "**NB:** the solutions to Exercises are provided in a separate tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPGW8XTgWV5w"
      },
      "source": [
        "## Part 1. Static Word Embeddings\n",
        "\n",
        "Before using pretrained language models, we will first learn about what word embeddings are and how they can represent useful information about the meanings and grammatical properties of words.\n",
        "\n",
        "Word embeddings represent words as points in a continuous, multi-dimensional **semantic space**, where distance reflects similarity of meaning or usage. Words that occur in similar contexts (for example, *king* and *queen*) tend to be close together in this space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU1PvX_xCGjq"
      },
      "source": [
        "### Setup\n",
        "> While the environment is being set up and required files are downloading,  \n",
        "> please continue reading the instructions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kc_KCzVEGTPS"
      },
      "outputs": [],
      "source": [
        "# --- Install required libraries ---\n",
        "!pip install -q numpy pandas matplotlib seaborn scikit-learn plotly wget\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYR--4VUGi9W"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, numpy as np, pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt, seaborn as sns, plotly.express as px\n",
        "import wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "37LpZgDfp6P7"
      },
      "outputs": [],
      "source": [
        "#@title Helper functions (part 1)\n",
        "# --- Load GloVe static embeddings ---\n",
        "def load_glove_model(dim=100):\n",
        "    import os, wget, numpy as np, pathlib\n",
        "    \"\"\"\n",
        "    Downloads and loads GloVe vectors of chosen dimensionality.\n",
        "    dim ‚àà {50, 100, 200, 300}\n",
        "    \"\"\"\n",
        "    url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "    glove_dir = pathlib.Path(\"glove.6B\")\n",
        "    glove_file = glove_dir / f\"glove.6B.{dim}d.txt\"\n",
        "\n",
        "    # Download and unzip if necessary\n",
        "    if not glove_file.exists():\n",
        "        print(\"üì¶ Downloading GloVe embeddings...\")\n",
        "        wget.download(url)\n",
        "        !unzip -q glove.6B.zip -d glove.6B\n",
        "\n",
        "    print(f\"\\n‚úÖ Loading {glove_file.name} ...\")\n",
        "    embeddings = {}\n",
        "    with open(glove_file, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            word, vec = parts[0], np.array(parts[1:], dtype=float)\n",
        "            embeddings[word] = vec\n",
        "    print(f\"Loaded {len(embeddings):,} word vectors ({dim} dimensions).\")\n",
        "    return embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKiHmvGNry44"
      },
      "outputs": [],
      "source": [
        "# load glove model use helper function\n",
        "# this will take a few minute. You can also consider reducing the dimensions to save time.\n",
        "model_glove = load_glove_model(dim=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMRNBq4XoqPd"
      },
      "source": [
        "### 1.1 Word embeddings with pen and paper\n",
        "\n",
        "Consider the following words:\n",
        "\n",
        "**boy, girl, man, woman, king, queen, prince, princess**\n",
        "\n",
        "With pen and paper, draw two ‚Äúsemantic space‚Äù - one being 2D, and one being 3D - that could represent distinctions between these words.  \n",
        "- What dimensions might capture the main differences (for example, gender, age, or social status)?  \n",
        "- Where would each word be positioned in the 2D/3D space?  \n",
        "- How close would *king* and *queen* be compared to *boy* and *girl*?  \n",
        "\n",
        "**You can assume that a word such as *father* would be ‚Äúnon-royal.‚Äù*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHj1bKNEpn-g"
      },
      "source": [
        "### 1.2 Word embeddings from a computational model\n",
        "\n",
        "Now that we have a conceptual sense of what a semantic space might look like, we can examine how computers learn similar spaces automatically.\n",
        "\n",
        "We will start with **GloVe**, a classic example of *static word embeddings*.  Static embeddings assign one fixed vector to each word, regardless of the context in which it appears. Words with similar meanings occupy nearby positions in this vector space.\n",
        "\n",
        "**About GloVe**\n",
        "\n",
        "GloVe (*Global Vectors for Word Representation*) was trained on large text corpora such as Wikipedia and Gigaword using global word co-occurrence statistics. The model is based on the principle that words that frequently occur in similar contexts will have similar meanings.\n",
        "\n",
        "\n",
        "**In this section, we will:**\n",
        "1. Load pretrained GloVe embeddings.  \n",
        "2. Extract vectors for a small set of words.  \n",
        "3. Visualize these high-dimensional vectors in a 2-dimensional space using principal component analysis (PCA).  \n",
        "4. Interpret how semantic relationships such as gender or royalty appear geometrically in the resulting space.\n",
        "\n",
        "**About PCA in Word Embeddings**  \n",
        "\n",
        "Word embeddings are high-dimensional ‚Äî each word is represented by a vector of dozens or hundreds of numbers.  \n",
        "While these vectors encode rich information about meaning and relationships, they exist in a space that‚Äôs difficult to visualize directly.  \n",
        "\n",
        "To explore this structure, we use **Principal Component Analysis (PCA)** ‚Äî a dimensionality reduction technique that projects high-dimensional data onto a smaller number of axes (typically 2 or 3) that capture the greatest amount of variance in the data.  \n",
        "\n",
        "PCA works by finding new, **orthogonal directions (principal components)** in the data that explain as much variation as possible. By keeping only the top few components, we can summarize the global structure of the embedding space in a small number of interpretable dimensions.  \n",
        "\n",
        "This kind of visualization doesn‚Äôt capture every nuance of the full space, but it gives us an **intuitive map** of how meaning is organized ‚Äî making abstract numerical relationships more interpretable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RCl7tE7wsiU"
      },
      "outputs": [],
      "source": [
        "# Define the words to explore\n",
        "words = [\"boy\", \"girl\", \"man\", \"woman\", \"king\", \"queen\", \"prince\", \"princess\"]\n",
        "\n",
        "# Retrieve their vector representations\n",
        "word_vecs = np.array([model_glove[w] for w in words])\n",
        "\n",
        "# --- Create a DataFrame for convenience ---\n",
        "static_embeddings = pd.DataFrame(word_vecs, index=words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlSq8D5uwuyj"
      },
      "outputs": [],
      "source": [
        "# --- Dimensionality reduction for visualization (2D PCA) ---\n",
        "pca = PCA(n_components=2)\n",
        "coords = pca.fit_transform(static_embeddings)\n",
        "data_pca = pd.DataFrame(coords, columns=['PC1', 'PC2'], index=words)\n",
        "\n",
        "# --- Plot in 2D space ---\n",
        "fig = px.scatter(\n",
        "    data_pca,\n",
        "    x='PC1',\n",
        "    y='PC2',\n",
        "    text=data_pca.index,\n",
        "    title=\"Semantic Space of Static Word Embeddings (GloVe)\",\n",
        "    width=700,\n",
        "    height=500\n",
        ")\n",
        "fig.update_traces(textposition='top center', marker=dict(size=10))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F02-zvOhaVUO"
      },
      "source": [
        "## Exercise 1. Exploring Static Word Embeddings\n",
        "\n",
        "### 1. Inspect the 2-D semantic space\n",
        "Examine where each word is positioned in your PCA plot.  \n",
        "- Think about\n",
        "  - Do similar words (e.g., *man‚Äìwoman*, *king‚Äìqueen*) cluster together?  \n",
        "  - Does the overall layout resemble the conceptual 2D space you sketched earlier in the **pen-and-paper exercise**?  \n",
        "- Try giving each axis a **conceptual label**.\n",
        "\n",
        "\n",
        "### 2. Visualize in 3-D\n",
        "- Re-run your analysis with **three principal components** and create a 3-D scatter plot.  \n",
        "- Think about\n",
        "  - Does the extra dimension reveal new patterns?  \n",
        "  - Does the overall layout resemble the conceptual 3D space you sketched earlier in the **pen-and-paper exercise**?  \n",
        "  - Are the relationships between words easier or harder to interpret?\n",
        "\n",
        "\n",
        "### 3. (Optional) Compare other static embedding models\n",
        "- Explore how different pre-trained **static word embeddings** capture meaning (e.g., Word2Vec (skip-gram, CBOW), fastText).\n",
        "- Compare how the **positions and clusters** of words differ across these embeddings. Which relationships remain consistent? Which ones change?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cDq_M0zXyEt"
      },
      "source": [
        "## Part 2. Contextualized Word Embeddings\n",
        "In this section, we will work with **BERT**, a transformer-based language model that learns *context-dependent* representations of words.\n",
        "\n",
        "Unlike GloVe, which assigns a single fixed vector to each word, BERT dynamically adjusts a word‚Äôs embedding depending on its surrounding context. The same word can therefore have different representations in different sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9swsZ78iD3DN"
      },
      "source": [
        "### Setup\n",
        "> GloVe uses only lightweight libraries (like NumPy and scikit-learn), while BERT and the fMRI analyses require additional, heavier packages, such as PyTorch, Transformers, and Nilearn.  \n",
        ">\n",
        "> To keep the tutorial clean and avoid version or memory conflicts,  \n",
        "> we‚Äôll reset the Colab environment before moving to Part 2.\n",
        ">\n",
        "> **To reset:**\n",
        "> 1. Go to the menu: **Runtime ‚Üí Restart session**  \n",
        "> 2. After the restart, re-run the setup cell below to initialize the new environment.\n",
        "\n",
        "\n",
        "> While the environment is being set up and required files are downloading,  \n",
        "> please continue reading the instructions below.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade pip\n",
        "!pip install -q torch transformers scipy wget\n",
        "!pip install -q numpy pandas seaborn matplotlib\n"
      ],
      "metadata": {
        "id": "GtC8r-xtzmcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Standard Library ---\n",
        "import os\n",
        "import math\n",
        "import copy\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "\n",
        "# --- Scientific / Numeric ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import cosine, euclidean, pdist, squareform, cdist\n",
        "\n",
        "# --- Deep Learning / NLP ---\n",
        "import torch\n",
        "import transformers\n",
        "from torch.nn import functional as F\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# --- Visualization ---\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# --- Utilities ---\n",
        "import wget\n"
      ],
      "metadata": {
        "id": "J2sqJdO7EvF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3dT1qUi6TEL4"
      },
      "outputs": [],
      "source": [
        "#@title Helper functions (part 2 and 3)\n",
        "def get_vectors(model, tokenizer, texts):\n",
        "  \"\"\"\n",
        "  Return vector embeddings for each text in texts, for each layer of the model.\n",
        "  \"\"\"\n",
        "  text_words = []\n",
        "  text_vecs = defaultdict(list)\n",
        "\n",
        "  for text in texts:\n",
        "    encoded = tokenizer(text, return_tensors=\"pt\")\n",
        "    input_ids = encoded['input_ids']\n",
        "    attention_mask = encoded['attention_mask']\n",
        "    output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    states = output.hidden_states\n",
        "    token_len = attention_mask.sum().item()\n",
        "    decoded = tokenizer.convert_ids_to_tokens(input_ids[0], skip_special_tokens=False)\n",
        "    if \"GPT2\" in str(model):\n",
        "      word_indices = np.array(list(map(lambda e: -1 if e is None else e, encoded.word_ids())))[:token_len]\n",
        "      word_groups = np.split(np.arange(word_indices.shape[0]), np.unique(word_indices, return_index=True)[1])[1:]\n",
        "      text_words.append([\"\".join(list(map(lambda t: t[1:] if t[:1] == \"ƒ†\" else t, np.array(decoded)[g]))) for g in word_groups])\n",
        "      num_layers = len(model.base_model.h)\n",
        "      emb_layer = model.wte\n",
        "    elif \"Bert\" in str(model):\n",
        "      word_indices = np.array(list(map(lambda e: -1 if e is None else e, encoded.word_ids())))[1:token_len - 1]\n",
        "      word_groups = np.split(np.arange(word_indices.shape[0]) + 1, np.unique(word_indices, return_index=True)[1])[1:]\n",
        "      text_words.append([\"\".join(list(map(lambda t: t[2:] if t[:2] == \"##\" else t, np.array(decoded)[g]))) for g in word_groups])\n",
        "      if \"DistilBert\" in str(model):\n",
        "        num_layers = len(model.base_model.transformer.layer)\n",
        "      else:\n",
        "        num_layers = len(model.base_model.encoder.layer)\n",
        "      emb_layer = model.embeddings.word_embeddings\n",
        "    else:\n",
        "      return NotImplementedError\n",
        "\n",
        "    for layer in range(0,num_layers+1):\n",
        "      text_tokens_output = torch.stack([\n",
        "          torch.stack([\n",
        "              # model layer embedding\n",
        "              states[layer].detach()[:, token_ids_word].mean(axis=1)\n",
        "              if layer > 0 else\n",
        "              # input embedding\n",
        "              emb_layer(input_ids)[:, token_ids_word].mean(axis=1)\n",
        "              ]).sum(axis=0).squeeze()\n",
        "              for token_ids_word in word_groups\n",
        "      ])\n",
        "      text_vecs[layer].append(text_tokens_output.numpy())\n",
        "\n",
        "  for layer in range(0,num_layers+1):\n",
        "    text_vecs[layer] = np.vstack(text_vecs[layer])\n",
        "\n",
        "  return text_vecs\n",
        "\n",
        "def compute_distance_matrices(vecs, measure):\n",
        "  distance_matrices = [\n",
        "    cdist(vecs[i], vecs[i], measure).round(10)\n",
        "        for i in range(len(vecs))\n",
        "  ]\n",
        "  return distance_matrices\n",
        "\n",
        "def plot_sentence_RDMs(sentences, distance_matrices, model_name, tokenizer):\n",
        "  labels = [tokenizer.convert_ids_to_tokens(ids) for ids in tokenizer(sentences)['input_ids']][0][1:-1]\n",
        "\n",
        "  plot_data = [\n",
        "      (\"xy\", 0, \"input embeddings\"),\n",
        "      (\"x\", 1, \"layer 1\"),\n",
        "      (\"xy\", 5, \"layer 5\"),\n",
        "      (\"x\", 11, \"layer 11\"),\n",
        "  ]\n",
        "\n",
        "  fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(18, 24))\n",
        "  fig.suptitle(model_name + ': distance matrices between word embeddings', y=0.9, weight='bold')\n",
        "\n",
        "  for subplot, (label_axes, matrix_index, title) in zip(itertools.chain.from_iterable(axes), plot_data):\n",
        "      heatmap_args = dict(\n",
        "          linewidth=1,\n",
        "          annot = np.array([[f\"{v:.2f}\" if (v == 0 or len(str(v)) > 4)\n",
        "          else f\"{v:.1f}\" for v in r] for r in distance_matrices[matrix_index]]),\n",
        "          annot_kws={\"size\":6.5},\n",
        "          fmt=\"\",\n",
        "          cmap = 'magma_r',\n",
        "          xticklabels=labels,\n",
        "          yticklabels=labels,\n",
        "      )\n",
        "\n",
        "      heatmap = sns.heatmap(distance_matrices[matrix_index], ax=subplot, **heatmap_args)\n",
        "      subplot.set_title(title)\n",
        "      for axis in [x for x in \"xy\" if x not in label_axes]:\n",
        "          getattr(subplot, f\"{axis}axis\").set_visible(False)\n",
        "      subplot.set_yticklabels(labels, rotation=0)\n",
        "      subplot.set_xticklabels(labels, rotation=90)\n",
        "  plt.show()\n",
        "\n",
        "def my_correlation_rsa(DM1, DM2, method='spearman'):\n",
        "    \"\"\"\n",
        "    Compute representational similarity between two distance matrices.\n",
        "    \"\"\"\n",
        "    # selection elements of the upper triangle\n",
        "    elements1 = DM1[np.triu_indices(DM1.shape[1],k=1)]\n",
        "    elements2 = DM2[np.triu_indices(DM2.shape[1],k=1)]\n",
        "\n",
        "    # compute correlation\n",
        "    if method == 'pearson':\n",
        "        correlation_of_similarities = stats.pearsonr(elements1, elements2)\n",
        "    elif method == 'spearman':\n",
        "        correlation_of_similarities = stats.spearmanr(elements1, elements2)\n",
        "    else:\n",
        "        return NotImplementedError\n",
        "\n",
        "    return correlation_of_similarities\n",
        "\n",
        "def RSA_matrix(distance_matrices, method='spearman'):\n",
        "    # create the matrix to fill with the results\n",
        "    result_matrix = np.ones((len(distance_matrices), len(distance_matrices)))\n",
        "    for left_ix, right_ix in itertools.combinations(range(len(distance_matrices)), 2):\n",
        "        left = distance_matrices[left_ix]\n",
        "        right = distance_matrices[right_ix]\n",
        "        # compute RS\n",
        "        correlation, p_value = my_correlation_rsa(left, right, method=method)\n",
        "        # put the result in the matrix\n",
        "        result_matrix[left_ix][right_ix] = correlation\n",
        "        # (optionally) also in the other triangle\n",
        "        result_matrix[right_ix][left_ix] = correlation\n",
        "    return result_matrix\n",
        "\n",
        "def plot_RSA(result_matrix, model_name, dist_method):\n",
        "    layer_labels = [\"input embedding\"] + [\"layer \" + str(l) for l in range(1, len(result_matrix))]\n",
        "    plt.figure(figsize = (10,6))\n",
        "    ax = sns.heatmap(result_matrix, annot = True,\n",
        "                    cmap = 'magma_r',\n",
        "                    xticklabels=layer_labels,\n",
        "                    yticklabels=layer_labels )\n",
        "    ax.set_title(f'RSA: {model_name} embeddings across layers ({dist_method})')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# --- Supported Models ---\n",
        "supported_models = [\"bert-base-uncased\", \"gpt2\"]\n",
        "\n",
        "\n",
        "# --- Data Loading & Extraction ---\n",
        "def load_subj_dict(filepath: str) -> dict:\n",
        "    \"\"\"Load one subject's .mat file into a structured Python dictionary.\"\"\"\n",
        "    subj_mat = scipy.io.loadmat(filepath)\n",
        "    subj_dict = {\"meta\": {n: [] for n in subj_mat[\"meta\"].dtype.names},\n",
        "                 \"words\": {n: [] for n in subj_mat[\"words\"].dtype.names}}\n",
        "\n",
        "    subj_dict[\"time\"] = subj_mat[\"time\"].squeeze()\n",
        "    subj_dict[\"data\"] = subj_mat[\"data\"].squeeze()\n",
        "    subj_dict[\"meta\"][\"subject\"] = int(subj_mat[\"meta\"][\"subject\"].squeeze())\n",
        "    subj_dict[\"meta\"][\"nTRs\"] = int(subj_mat[\"meta\"][\"nTRs\"].squeeze())\n",
        "    subj_dict[\"meta\"][\"nvoxels\"] = int(subj_mat[\"meta\"][\"nvoxels\"].squeeze())\n",
        "    subj_dict[\"meta\"][\"ncols\"] = subj_dict[\"data\"].shape[1]\n",
        "    subj_dict[\"meta\"][\"dimx\"] = int(subj_mat[\"meta\"][\"dimx\"].squeeze())\n",
        "    subj_dict[\"meta\"][\"dimy\"] = int(subj_mat[\"meta\"][\"dimy\"].squeeze())\n",
        "    subj_dict[\"meta\"][\"dimz\"] = int(subj_mat[\"meta\"][\"dimz\"].squeeze())\n",
        "    subj_dict[\"meta\"][\"colToCoord\"] = subj_mat[\"meta\"][\"colToCoord\"].squeeze()[()]\n",
        "    subj_dict[\"meta\"][\"coordToCol\"] = subj_mat[\"meta\"][\"coordToCol\"].squeeze()[()]\n",
        "    subj_dict[\"meta\"][\"colToROInum\"] = subj_mat[\"meta\"][\"colToROInum\"].squeeze()[()].flatten()\n",
        "    subj_dict[\"meta\"][\"coordToROInum\"] = subj_mat[\"meta\"][\"coordToROInum\"].squeeze()[()]\n",
        "    subj_dict[\"meta\"][\"ROInumToName\"] = np.array(\n",
        "        [t.squeeze()[()] for t in subj_mat[\"meta\"][\"ROInumToName\"].squeeze()[()].flatten()]\n",
        "    )\n",
        "    subj_dict[\"meta\"][\"voxel_size\"] = subj_mat[\"meta\"][\"voxel_size\"].squeeze()[()].flatten()\n",
        "    subj_dict[\"meta\"][\"matrix\"] = subj_mat[\"meta\"][\"matrix\"].squeeze()[()]\n",
        "    subj_dict[\"words\"][\"text\"] = np.array(\n",
        "        [t.squeeze()[()].item() for t in subj_mat[\"words\"][\"text\"].flatten()]\n",
        "    )\n",
        "    subj_dict[\"words\"][\"start\"] = np.array(\n",
        "        [t.squeeze()[()].item() for t in subj_mat[\"words\"][\"start\"].flatten()]\n",
        "    )\n",
        "    subj_dict[\"words\"][\"length\"] = np.array(\n",
        "        [t.squeeze()[()].item() for t in subj_mat[\"words\"][\"length\"].flatten()]\n",
        "    )\n",
        "\n",
        "    subj_dict[\"meta\"][\"coordToCol\"] = np.where(\n",
        "        subj_dict[\"meta\"][\"coordToCol\"] == 0, None, subj_dict[\"meta\"][\"coordToCol\"] - 1\n",
        "    )\n",
        "    subj_dict[\"meta\"][\"ROInumToName\"] = np.insert(\n",
        "        subj_dict[\"meta\"][\"ROInumToName\"][:-1], 0, subj_dict[\"meta\"][\"ROInumToName\"][-1]\n",
        "    )\n",
        "    return subj_dict\n",
        "\n",
        "\n",
        "def text_between(start_time: float, end_time: float, subj_dict: dict) -> list:\n",
        "    words = subj_dict[\"words\"]\n",
        "    idx = np.argwhere((words[\"start\"] >= start_time) & (words[\"start\"] < end_time))\n",
        "    text = [w[0] for w in words[\"text\"][idx]]\n",
        "    return text\n",
        "\n",
        "\n",
        "def text_at_tr(tr: int, subj_dict: dict) -> list:\n",
        "    time = subj_dict[\"time\"]\n",
        "    final_tr = len(time) - 1\n",
        "    if tr < final_tr:\n",
        "        tr_end = time[tr + 1, 0]\n",
        "    elif tr == final_tr:\n",
        "        tr_end = time[tr, 0]\n",
        "    else:\n",
        "        raise ValueError(f\"TR {tr} does not exist, final TR is {final_tr}.\")\n",
        "    tr_start = time[tr, 0]\n",
        "    return text_between(tr_start, tr_end, subj_dict)\n",
        "\n",
        "\n",
        "def get_text_TRs(subj_dict: dict):\n",
        "    tr_texts = np.array([\" \".join(text_at_tr(tr, subj_dict)) for tr in range(subj_dict[\"meta\"][\"nTRs\"])])\n",
        "    texts = np.array([t for t in tr_texts if t != \"\"])\n",
        "    text_TR_idx = np.where(tr_texts != \"\")[0]\n",
        "    return (text_TR_idx, texts)\n",
        "\n",
        "\n",
        "def data_for_ROI(ROI: list, subj_dict: dict) -> np.ndarray:\n",
        "    col2ROIname = subj_dict[\"meta\"][\"ROInumToName\"][subj_dict[\"meta\"][\"colToROInum\"]]\n",
        "    ROI_cols = np.where(np.isin(col2ROIname, ROI))\n",
        "    ROI_signal = subj_dict[\"data\"][:, ROI_cols].squeeze()\n",
        "    return ROI_signal\n",
        "\n",
        "\n",
        "def get_text_response_scans(subj_dict: dict, delay: int = 2, ROI: list = None) -> dict:\n",
        "    voxel_signals = data_for_ROI(ROI, subj_dict) if ROI else subj_dict[\"data\"]\n",
        "    text_TR_idx, texts = get_text_TRs(subj_dict)\n",
        "    brain_responses = voxel_signals[text_TR_idx + delay]\n",
        "    blocks = subj_dict[\"time\"][:, 1][text_TR_idx + delay]\n",
        "    return {\n",
        "        \"scan_TR_idx\": text_TR_idx + delay,\n",
        "        \"text_TR_idx\": text_TR_idx,\n",
        "        \"texts\": texts,\n",
        "        \"blocks\": blocks,\n",
        "        \"voxel_signals\": brain_responses,\n",
        "    }\n",
        "\n",
        "\n",
        "# --- Text & Embeddings ---\n",
        "def create_context_sentences(tr_texts: list) -> list:\n",
        "    fulltext = \" \".join(tr_texts)\n",
        "    nlp = English()\n",
        "    nlp.add_pipe(\"sentencizer\")\n",
        "    sentences = [str(s).split(\" \") for s in list(nlp(fulltext).sents)]\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        if sentences[i][-1] == '\"':\n",
        "            del sentences[i][-1]\n",
        "            sentences[i + 1][0] = '\"' + sentences[i + 1][0]\n",
        "        if sentences[i][0] == \"+\":\n",
        "            del sentences[i][0]\n",
        "            sentences[i - 1].append(\"+\")\n",
        "\n",
        "    if not sentences[-1]:\n",
        "        del sentences[-1]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def get_tr_embeddings(layer_activations, words_per_tr):\n",
        "    start_word = 0\n",
        "    tr_embeddings = []\n",
        "    for n_words in words_per_tr:\n",
        "        tr_features = layer_activations[start_word : start_word + n_words]\n",
        "        tr_embeddings.append(np.mean(tr_features, axis=0))\n",
        "        start_word += n_words\n",
        "    return np.array(tr_embeddings)\n",
        "\n",
        "\n",
        "# --- Torch Dataset & Batching ---\n",
        "class WordIDsDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset class to keep token-word mappings for transformer models.\"\"\"\n",
        "\n",
        "    def __init__(self, sents: transformers.tokenization_utils_base.BatchEncoding):\n",
        "        self.sents = sents\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.sents[\"input_ids\"].size(0)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        sent = self.sents[idx]\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(sent.ids),\n",
        "            \"word_ids\": torch.tensor(list(map(lambda e: -1 if e is None else e, sent.word_ids))),\n",
        "            \"attention_mask\": torch.tensor(sent.attention_mask),\n",
        "        }\n",
        "\n",
        "\n",
        "def get_batches(input_dict, batch_size=1):\n",
        "    tensor_dataset = WordIDsDataset(input_dict)\n",
        "    return torch.utils.data.DataLoader(tensor_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# --- Model Handling ---\n",
        "def model_init(model_identifier):\n",
        "    assert model_identifier in supported_models, f\"model_identifier must be one of {supported_models}\"\n",
        "    model = AutoModel.from_pretrained(model_identifier, output_hidden_states=True, output_attentions=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_identifier)\n",
        "    if \"GPT2Model\" in str(model):\n",
        "        tokenizer.add_prefix_space = True\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def get_layer_activations(model, tokenizer, input_texts):\n",
        "    encoded_texts = tokenizer.batch_encode_plus(\n",
        "        input_texts, is_split_into_words=True, padding=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    dl = get_batches(encoded_texts)\n",
        "    texts_activations = []\n",
        "\n",
        "    for batch, input_dict in enumerate(dl):\n",
        "        word_ids = input_dict.pop(\"word_ids\")\n",
        "        input_ids = input_dict[\"input_ids\"]\n",
        "        attention_mask = input_dict[\"attention_mask\"]\n",
        "        token_len = attention_mask.sum().item()\n",
        "\n",
        "        if \"BertModel\" in str(model):\n",
        "            word_indices = np.array([ -1 if e is None else e for e in word_ids.numpy().squeeze()])[1 : token_len - 1]\n",
        "            word_groups = np.split(np.arange(word_indices.shape[0]) + 1, np.unique(word_indices, return_index=True)[1])[1:]\n",
        "            input_token_embeddings = model.embeddings.word_embeddings(input_ids)\n",
        "\n",
        "        elif \"GPT2Model\" in str(model):\n",
        "            word_indices = np.array([ -1 if e is None else e for e in word_ids.numpy().squeeze()])[:token_len]\n",
        "            word_groups = np.split(np.arange(word_indices.shape[0]), np.unique(word_indices, return_index=True)[1])[1:]\n",
        "            input_token_embeddings = model.wte(input_ids)\n",
        "        else:\n",
        "            raise NotImplementedError(\"only supports BERT or GPT2 models\")\n",
        "\n",
        "        model_output = model(**input_dict)\n",
        "        layer_activations = torch.stack(model_output.hidden_states)\n",
        "        layer_activations_per_word = torch.stack([\n",
        "            torch.stack([\n",
        "                layer_activations[i, 0, token_ids_word, :].mean(axis=0)\n",
        "                if i > 0 else input_token_embeddings[0, token_ids_word, :].mean(axis=0)\n",
        "                for i in range(len(model_output.hidden_states))\n",
        "            ]) for token_ids_word in word_groups\n",
        "        ])\n",
        "\n",
        "        texts_activations.append(layer_activations_per_word.detach().numpy())\n",
        "    return texts_activations\n",
        "\n",
        "\n",
        "# --- RSA and Distances ---\n",
        "def vector_distance_matrix(activations, metric=\"cosine\"):\n",
        "    return squareform(pdist(activations, metric=metric))\n",
        "\n",
        "\n",
        "def string_distance_matrix(text, normalize=False):\n",
        "    dist_mat = np.zeros((len(text), len(text)))\n",
        "    for i in range(len(text)):\n",
        "        for j in range(len(text)):\n",
        "            dist_mat[i, j] = edit_distance(text[i], text[j])\n",
        "    if normalize:\n",
        "        dist_mat /= np.linalg.norm(dist_mat)\n",
        "    return dist_mat\n",
        "\n",
        "\n",
        "def compute_rsa_score(RDM_1, RDM_2, score=\"pearsonr\"):\n",
        "    pdists1 = squareform(RDM_1)\n",
        "    pdists2 = squareform(RDM_2)\n",
        "    if score != \"pearsonr\":\n",
        "        raise NotImplementedError(\"currently only supporting pearsonr similarity score\")\n",
        "    rsa_score, _ = pearsonr(pdists1, pdists2)\n",
        "    return rsa_score\n",
        "\n",
        "\n",
        "def rsa_matrix(RDMs_1, RDMs_2, score=\"pearsonr\"):\n",
        "    rsa_mat = np.zeros((len(RDMs_1), len(RDMs_2)))\n",
        "    for i in range(len(RDMs_1)):\n",
        "        for j in range(len(RDMs_2)):\n",
        "            rsa_mat[i, j] = compute_rsa_score(RDMs_1[i], RDMs_2[j], score=score)\n",
        "    return rsa_mat\n",
        "\n",
        "def top_n_next_tokens(vocab_probs, tokenizer, top_n):\n",
        "  # top N most likely token ids\n",
        "  next_token_ids = np.argsort(vocab_probs)[0][-top_n:]\n",
        "  # find the tokens for these ids\n",
        "  next_tokens = tokenizer.decode(next_token_ids).split(' ')[1:]\n",
        "  # find the probabilities\n",
        "  next_token_probs = list(vocab_probs[:, next_token_ids][0].numpy())\n",
        "  # sort the tokens and their probabilities in descending order\n",
        "  desc_token_probs = list(zip(next_tokens, next_token_probs))[::-1]\n",
        "  return desc_token_probs\n",
        "\n",
        "\n",
        "# --- Load GloVe static embeddings ---\n",
        "def load_glove_model(dim=100):\n",
        "    import os, wget, numpy as np, pathlib\n",
        "    \"\"\"\n",
        "    Downloads and loads GloVe vectors of chosen dimensionality.\n",
        "    dim ‚àà {50, 100, 200, 300}\n",
        "    \"\"\"\n",
        "    url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "    glove_dir = pathlib.Path(\"glove.6B\")\n",
        "    glove_file = glove_dir / f\"glove.6B.{dim}d.txt\"\n",
        "\n",
        "    # Download and unzip if necessary\n",
        "    if not glove_file.exists():\n",
        "        print(\"üì¶ Downloading GloVe embeddings...\")\n",
        "        wget.download(url)\n",
        "        !unzip -q glove.6B.zip -d glove.6B\n",
        "\n",
        "    print(f\"\\n‚úÖ Loading {glove_file.name} ...\")\n",
        "    embeddings = {}\n",
        "    with open(glove_file, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            word, vec = parts[0], np.array(parts[1:], dtype=float)\n",
        "            embeddings[word] = vec\n",
        "    print(f\"Loaded {len(embeddings):,} word vectors ({dim} dimensions).\")\n",
        "    return embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Igkrp24qH1L"
      },
      "source": [
        "### 2.1 Static and contextualized representations in large language models\n",
        "\n",
        "In large language models such as BERT, the **input embeddings** are static‚Äîeach word or token has its own unique input vector. However, these input vectors are transformed through multiple layers of the model. The **activation patterns** (or output vectors) of the higher layers capture richer, context-dependent information.  \n",
        "\n",
        "When we use these vectors as representations for words, we refer to them as **contextualized embeddings**. These embeddings often provide more useful information for downstream analyses, such as predicting brain activity or behavioral measures.\n",
        "\n",
        "**In this section, we will:**\n",
        "1. Load a pretrained BERT model and tokenizer.\n",
        "2. Extract embeddings from different layers.\n",
        "3. Compute pairwise cosine distances between word representations.\n",
        "4. Visualize the resulting **representational dissimilarity matrices (RDMs)**.\n",
        "\n",
        "\n",
        "**About BERT**\n",
        "\n",
        "BERT (Bidirectional encoder representations from transformers) was the first successful **transformer-based** model and remains one of the most influential architectures in modern NLP (natural language processing).\n",
        "\n",
        "BERT is a **masked language model**: it is trained to fill in missing words in a sentence given both the preceding and following context.  \n",
        "For example, during training, BERT sees inputs like:\n",
        "\n",
        "> \"The dog [MASK] loudly.\"\n",
        "\n",
        "and learns to predict the missing word (*barked*, or another plausible option) based on both *the dog* and *loudly*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqZYm61AFgoK"
      },
      "outputs": [],
      "source": [
        "# load the model and tokenizer\n",
        "model_name = \"bert-base-uncased\"  # standard 12-layer BERT model, trained on English text with all words lowercased.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(\n",
        "    model_name,\n",
        "    output_hidden_states=True,# the model will return all hidden layers‚Äô embeddings\n",
        "    output_attentions=True # the model will also return the attention weights from each layer\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "torch.set_grad_enabled(False) # disables training mode\n",
        "print(\"‚úÖ BERT model and tokenizer loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA-Gqt_kg6PJ"
      },
      "source": [
        "### 2.2 Representational similarity analysis\n",
        "\n",
        "We use methods such as **representational dissimilarity matrices (RDMs)** and **representational similarity analysis (RSA)** to understand the structure of different embedding spaces.\n",
        "\n",
        "An RDM summarizes how dissimilar each pair of representations is‚Äîoften measured using **correlation distance**.  \n",
        "\n",
        "By comparing RDMs across different model layers, we can examine how semantic relationships evolve from lower (more lexical) to higher (more contextual) layers of the model.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok_TdRJOqCxC"
      },
      "outputs": [],
      "source": [
        "# get embedding vectors for two different sentences (using helper function get_vectors)\n",
        "sentences = [\n",
        "    \"a computer needs a mouse. a cat eats a mouse.\"\n",
        "]\n",
        "# get a dictionary of embeddings per layer\n",
        "sentence_vecs = get_vectors(model, tokenizer, sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the embeddings\n",
        "print(f\"Total layers: {len(sentence_vecs)}\")\n",
        "print(f\"Embedding dimension: {sentence_vecs[0].shape[1]}\")\n",
        "print(f\"Tokens: {len(sentence_vecs[0])}\")\n",
        "tokens = tokenizer.tokenize(sentences[0])\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "R9sVEG4mOr8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QkYDlwQqKJy"
      },
      "outputs": [],
      "source": [
        "# compute distance matrices\n",
        "sentence_RDMs = compute_distance_matrices(sentence_vecs, measure=\"cosine\")\n",
        "\n",
        "# Inspect the RDM structure\n",
        "print(f\"Number of RDMs (model layers): {len(sentence_RDMs)}\")\n",
        "# Each RDM is a square matrix of token √ó token distances\n",
        "print(f\"Shape of one RDM: {sentence_RDMs[0].shape}\")\n",
        "# Check a small slice of the first RDM\n",
        "print(\"\\nExample RDM (first 5 tokens):\")\n",
        "print(np.round(sentence_RDMs[0][:5, :5], 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaaccwdCqLP7"
      },
      "outputs": [],
      "source": [
        "# plotting the RDMs using the helper codes plot_sentence_RDMs\n",
        "# The code shows a small, representative subet of layers (input, 1, 5, 11)\n",
        "plot_sentence_RDMs(sentences, sentence_RDMs, model_name, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Reflection\n",
        "- Look closely at the four heatmaps showing distance matrices between word embeddings across different BERT layers.\n",
        "- What information is captured by RDMs at different layers?"
      ],
      "metadata": {
        "id": "kVe1EqfTagOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interpretation and Take-away\n",
        "\n",
        "- Look closely at the four heatmaps showing distance matrices between word embeddings across different BERT layers.\n",
        "  - Each heatmap is a distance matrix between embeddings of the words in two similar sentences:\n",
        "  - The distance value (0 ‚Üí yellow = similar; 1 ‚Üí purple/black = dissimilar) shows how far apart the word representations are in that layer.\n",
        "\n",
        "- What information is captured by RDMs at different layers?\n",
        "  - At the input layer, ‚Äúmouse‚Äù (in computer context) and ‚Äúmouse‚Äù (in cat context) are identical ‚Äî the model hasn‚Äôt seen context yet.\n",
        "‚Üí They share exactly the same token embedding.\n",
        "  - As we move to higher layers, the embeddings become contextualized:\n",
        "The model differentiates ‚Äúmouse (device)‚Äù vs ‚Äúmouse (animal)‚Äù by integrating sentence-level meaning.\n",
        "  - However, in very high layers (like layer 11‚Äì12), these representations may reconverge somewhat, reflecting abstract or task-agnostic meaning integration ‚Äî they encode not only lexical semantics but also sentence-level information."
      ],
      "metadata": {
        "id": "ScrVeiBJSbea"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAPQIcHYoD8E"
      },
      "source": [
        "## Exercise 2: From Static to Contextual Meaning\n",
        "\n",
        "In this exercise, you‚Äôll explore how BERT represents words differently from static embedding models like GloVe.  \n",
        "You‚Äôll first compare **BERT‚Äôs input layer** (static-like) to GloVe, and then investigate how **contextual embeddings** evolve across sentences with different meanings of the same word.\n",
        "\n",
        "\n",
        "### 1. Static-like Representations in BERT\n",
        "\n",
        "BERT‚Äôs **input layer** (layer 0) contains one fixed vector per token type ‚Äî much like GloVe ‚Äî while deeper layers transform these embeddings based on surrounding context.\n",
        "\n",
        "1. Extract the **layer 0 embeddings** for the same set of words used in Part 1:  \n",
        "   *boy, girl, man, woman, king, queen, prince, princess*  \n",
        "2. Compute pairwise distances and visualize the relationships between words.\n",
        "\n",
        "**Think about:**\n",
        "- Do semantically related words (e.g., *king*‚Äì*queen*, *man*‚Äì*woman*) appear close together?  \n",
        "- How does this ‚Äústatic‚Äù space compare to the GloVe space from Part 1?  \n",
        "- What similarities or differences do you notice in the geometry of the embeddings?\n",
        "\n",
        "\n",
        "### 2. Context Shapes Meaning: The Case of *Mouse*\n",
        "\n",
        "Now test how BERT adjusts word meaning depending on context.  \n",
        "Use three sentences where *mouse* refers to different things:\n",
        "\n",
        "- ‚ÄúA computer uses a mouse.‚Äù  \n",
        "- ‚ÄúA cat chases a mouse.‚Äù  \n",
        "- ‚ÄúThe trap caught a mouse.‚Äù\n",
        "\n",
        "1. Extract the **top-layer embedding** of the final word *mouse* from each sentence.  \n",
        "2. Project the three *mouse* vectors into a 2-D PCA space and visualize them.\n",
        "\n",
        "**Think about:**\n",
        "- Do the two *animal-related* uses of *mouse* cluster together, separate from the *computer* sense?  \n",
        "- What might this reveal about how **context** and **syntactic role** influence BERT‚Äôs representations?  \n",
        "- Which layers do you think best capture **syntactic structure** vs. **semantic category**? (hint: take a look at the reflections above)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGv1kDTtqNXb"
      },
      "source": [
        "### 2.3 Understanding the RSA Across Layers of BERT\n",
        "\n",
        "Using these distance matrices, we can compare the representational geometry of **different BERT layers** using larger pieces of text. This helps us visualize how the model‚Äôs internal representations change from the input embeddings to deeper contextualized layers.\n",
        "\n",
        "**Input text**\n",
        "\n",
        "```text\n",
        "Harry had never believed he would meet a boy he hated more than Dudley, but that was before he met Draco Malfoy.  \n",
        "Still, first-year Gryffindors only had Potions with the Slytherins, so they didn't have to put up with Malfoy much.  \n",
        "Or at least, they didn't until they spotted a notice pinned up in the Gryffindor common room that made them all groan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX9VYlbMqN_7"
      },
      "outputs": [],
      "source": [
        "# input text for the model\n",
        "text = [\n",
        "    \"Harry had never believed he would meet a boy he hated more than Dudley, but that was before he met Draco Malfoy.\",\n",
        "    \"Still, first-year Gryffindors only had Potions with the Slytherins, so they didn't have to put up with Malfoy much.\",\n",
        "    \"Or at least, they didn't until they spotted a notice pinned up in the Gryffindor common room that made them all groan.\"\n",
        "    ]\n",
        "# get embedding vectors\n",
        "text_vecs = get_vectors(model, tokenizer, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf8xR7xvqPGj"
      },
      "outputs": [],
      "source": [
        "# Computing representational distance matrices (RDMs)\n",
        "distance_matrices_cosine = compute_distance_matrices(text_vecs, measure=\"cosine\")\n",
        "# Running RSA between layers\n",
        "# We then calculate the pairwise Pearson correlations (r) between the distance matrices across all layers.\n",
        "rsa_mat_cosine = RSA_matrix(distance_matrices_cosine, method=\"pearson\")\n",
        "# Visualizing the results\n",
        "plot_RSA(rsa_mat_cosine, model_name, dist_method=\"pearson's r\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reflection\n",
        "Read the figure carefully and reflect on\n",
        "- What pattern do you notice along the diagonal of the heatmap?  \n",
        "- How does the similarity between early and late layers change as we move down the network?  \n",
        "- What might it mean that deeper layers become less correlated with the input embedding?  \n"
      ],
      "metadata": {
        "id": "oo-cI81FZ4J7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eToQPW3zuSXE"
      },
      "source": [
        "#### Interpretation & Take-away\n",
        "\n",
        "- High similarity within neighboring layers (dark purple cells near the diagonal):\n",
        "\n",
        "  - Adjacent layers (e.g., Layer 2 vs. Layer 3, or 7 vs. 8) show very high correlation (‚âà0.9‚Äì0.97).\n",
        "\n",
        "  - This means that representations evolve smoothly across layers ‚Äî the model gradually refines information.\n",
        "\n",
        "- Progressive drift away from input embeddings:\n",
        "\n",
        "  - The correlation between the input embedding and early layers (0.7‚Äì0.8) is moderate, but it decreases sharply toward the upper layers (down to ‚âà0.2‚Äì0.3 by layer 12).\n",
        "\n",
        "  - This shows that BERT‚Äôs deeper layers capture more abstract, contextualized features that diverge from static input representations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3. Linking Language Models and the Brain\n",
        "\n",
        "In the first two parts of this tutorial, we explored how word representations evolve within language models:\n",
        "- **Part 1:** Static embeddings (e.g., GloVe) ‚Äî a single meaning per word.\n",
        "- **Part 2:** Contextual embeddings (e.g., BERT) ‚Äî meaning changes depending on context and deeper model layers.\n",
        "\n",
        "We saw that **deeper layers** in models like BERT capture increasingly **context-dependent** representations.  \n",
        "Now we ask:  \n",
        "> Do these deeper, more contextualized embeddings also become **more brain-like** compared to early, less contextualized embeddings?\n",
        "\n",
        "To address this question, We‚Äôll use **fMRI scans** recorded from one participant reading a chapter from *Harry Potter and the Sorcerer‚Äôs Stone* (Wehbe et al., 2014) and further explore the representational similarity analysis."
      ],
      "metadata": {
        "id": "RY6BtmJc-eTu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA5P2KgNy2l7"
      },
      "source": [
        "### Setup\n",
        "> While the environment is being set up and required files are downloading,  \n",
        "> please continue reading the instructions below.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This might take a few minutes\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q torch transformers numpy pandas scipy scikit-learn\n",
        "!pip install -q nilearn scprep h5py numba\n",
        "!pip install -q matplotlib seaborn plotly wget\n"
      ],
      "metadata": {
        "id": "HjFPtbKTiEYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Standard Library ---\n",
        "import os\n",
        "import math\n",
        "import copy\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "\n",
        "# --- Scientific / Numeric ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from scipy import stats\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.spatial.distance import cosine, euclidean, pdist, squareform, cdist\n",
        "\n",
        "# --- Deep Learning / NLP ---\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import transformers\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from nltk.metrics import edit_distance\n",
        "\n",
        "# --- fMRI / Neuroimaging ---\n",
        "import nilearn.signal\n",
        "import scprep\n",
        "import h5py\n",
        "\n",
        "# --- Visualization ---\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# --- Utilities ---\n",
        "import wget\n"
      ],
      "metadata": {
        "id": "RSlHgEui4ntY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the environment setup\n",
        "print(\"‚úÖ Environment ready\")\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Transformers:\", transformers.__version__)\n",
        "print(\"nilearn:\", nilearn.__version__)\n",
        "print(\"scprep:\", scprep.__version__)\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "id": "nBcaEf0_4svz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JTv-7KunNwm"
      },
      "outputs": [],
      "source": [
        "# download fMRI data from a open database (subject 8)\n",
        "subj_raw_file = 'subject_8.mat'\n",
        "if not os.path.exists(subj_raw_file):\n",
        "    url = 'http://www.cs.cmu.edu/~fmri/plosone/files/subject_8.mat'\n",
        "    wget.download(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDDfl3PsmC_B"
      },
      "outputs": [],
      "source": [
        "# download the TR text file\n",
        "tr_texts_file = 'tr_texts.txt'\n",
        "if not os.path.exists(tr_texts_file):\n",
        "  wget.download('https://raw.githubusercontent.com/clclab/ANCM/main/lab2/tr_texts.txt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NB:**\n",
        "you may need to rerun the helper functions in part 2"
      ],
      "metadata": {
        "id": "h98xyq5XCeRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Overview: Compare Models to the Brain\n",
        "In this part, we will use **Representational Similarity Analysis (RSA)** to compare model and brain representational spaces.\n",
        "\n",
        "Just as we compared **RDMs (Representational Dissimilarity Matrices)** across BERT layers earlier, we‚Äôll now compute the correlation between:\n",
        "- Model RDMs (from different layers), and  \n",
        "- Brain RDMs (from fMRI recordings).  \n",
        "\n",
        "This allows us to test whether later model layers form representational spaces **more similar to the brain**.\n",
        "\n",
        "\n",
        "**About the fMRI Dataset**\n",
        "\n",
        "We‚Äôll use **fMRI scans** recorded from one participant reading a chapter from  \n",
        "*Harry Potter and the Sorcerer‚Äôs Stone* (Wehbe et al., 2014).\n",
        "\n",
        "- **Stimulus presentation:** The text was shown word-by-word using **Rapid Serial Visual Presentation (RSVP)** ‚Äî each word appeared for **500 ms**.  \n",
        "- **Brain recording:** fMRI scans (3D brain volumes) were collected every **2 seconds (Time of Repetition, TR)**.  \n",
        "  ‚Üí Each scan thus corresponds roughly to the processing of **4 words**.\n",
        "\n",
        "To align model and brain data:\n",
        "- We‚Äôll **average the embeddings** over every 4 words to create **TR-level embeddings**.  \n",
        "- The code below provides these inputs which are already aligned for you:  \n",
        "  - `tr_texts.txt` ‚Äî 1295 TR texts (aligned with scans)  \n",
        "  - Corresponding **1295 brain responses** (voxel activity vectors).  \n",
        "\n",
        "- We‚Äôll **shift** brain activity by 4 seconds (2 TRs) to account for the **hemodynamic delay** (the lag between neural activity and the fMRI signal).  \n"
      ],
      "metadata": {
        "id": "z6M06smTcllS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Extract fMRI brain responses for the selected ROI\n",
        "\n",
        "We extract voxel-wise brain activity corresponding to each TR (2-second scan).\n",
        "\n",
        "**Region of Interest (ROI): Left Anterior Temporal Lobe (LATL)**\n",
        "\n",
        "- We‚Äôll start our analysis using fMRI data from the **Left Anterior Temporal Lobe (LATL)** ‚Äî a key brain area involved in **semantic processing**.\n",
        "- We define brain regions using the [AAL Single-Subject atlas](https://www.pmod.com/files/download/v36/doc/pneuro/6750.htm)\n",
        "- Later, you can define another selection of ROIs yourself, if you like (run `subj_dict['meta']['ROInumToName']` to see a list of all available ROIs in this dataset."
      ],
      "metadata": {
        "id": "ijp3ar_96xfh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQoKotymDZwl"
      },
      "outputs": [],
      "source": [
        "# load subject fMRI data into python dictionary\n",
        "subj_dict = load_subj_dict(subj_raw_file)\n",
        "\n",
        "# preprocess fMRI signals\n",
        "subj_cleaned_file = 'subject_8_clean.npy'\n",
        "if not os.path.exists(subj_cleaned_file):\n",
        "    # preprocessing parameters\n",
        "    cleaning_params = {\n",
        "        't_r': 2,                 # TR length in seconds\n",
        "        'low_pass': None,         # low-pass filter frequency cutoff (Hz)\n",
        "        'high_pass': 0.005,       # high-pass filter frequency cutoff (Hz)\n",
        "        'standardize': 'zscore',  # standardization method\n",
        "        'detrend': True,          # whether to apply detrending\n",
        "    }\n",
        "\n",
        "    cleaned_subj_dict = copy.copy(subj_dict)\n",
        "    cleaned_subj_dict['data'] = nilearn.signal.clean(subj_dict['data'],\n",
        "                                    runs=subj_dict['time'][:,1],\n",
        "                                    **cleaning_params)\n",
        "    np.save(subj_cleaned_file, cleaned_subj_dict)\n",
        "subj_dict = np.load(subj_cleaned_file, allow_pickle=True).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSxgQdASnTQC"
      },
      "outputs": [],
      "source": [
        "# subregions of left-anterior temporal lobe\n",
        "LATL_ROI = ['Temporal_Sup_L', 'Temporal_Pole_Sup_L',\n",
        "            'Temporal_Mid_L', 'Temporal_Pole_Mid_L', 'Temporal_Inf_L',\n",
        "            'Fusiform_L', 'ParaHippocampal_L']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piln_qtF9G3h"
      },
      "outputs": [],
      "source": [
        "# get brain responses\n",
        "# The delay=2 parameter aligns the model‚Äôs text input with the brain‚Äôs hemodynamic response (‚âà4s later).\n",
        "brain_responses = get_text_response_scans(subj_dict,\n",
        "                                          delay=2,\n",
        "                                          ROI=LATL_ROI) # delay in TRs (1 TR = 2 sec)\n",
        "# Get dimensions of the extracted data\n",
        "n_TRs = len(brain_responses[\"voxel_signals\"])\n",
        "n_voxels = brain_responses[\"voxel_signals\"].shape[1]\n",
        "\n",
        "print(f\"‚úÖ Extracted brain responses for {n_TRs} TRs \"\n",
        "      f\"({n_voxels} voxels in the LATL ROI).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Extract model emddings from BERT\n",
        "\n",
        "We now compute model representations aligned with the timing of the fMRI data.  Each TR (2 seconds) corresponds to a short text segment from the Harry Potter book chapter.  \n",
        "\n",
        "We will:\n",
        "1. Load and preprocess the TR texts,\n",
        "2. Extract contextualized embeddings for all words using BERT,\n",
        "3. Average the embeddings per TR to match brain scan timing.\n",
        "\n",
        "**Important caveat:**\n",
        "- We concatenate all TR texts into a continuous story and **split into sentences** (spaCy sentencizer). BERT was trained to use **bidirectional context** within a sentence (it is **non-causal**), so this step lets it encode each word with both preceding and following context.\n",
        "- Because BERT is non-causal, some TRs will be modeled with information from **words that appeared slightly later** in the story (e.g., the end of a sentence). This is acceptable for this analysis, but you can try a stricter variant later: feed BERT **only a fixed context window up to each TR** (i.e., a ‚Äúcausalized‚Äù approximation) and compare results.\n"
      ],
      "metadata": {
        "id": "-TVZ2aH6NajN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0swHZoKnXzz"
      },
      "outputs": [],
      "source": [
        "# Load and prepare TR-level text segments (‚âà4 words)\n",
        "# number of words per TR: this lets us later average model activations to the TR timescale.\n",
        "tr_texts = open(tr_texts_file, 'r').read().splitlines()\n",
        "words_per_tr = [len(tr.split(' ')) for tr in tr_texts] # count words per TR\n",
        "hp_sentences = create_context_sentences(tr_texts) # split text into sentences\n",
        "print(f\"‚úÖ Loaded {len(tr_texts)} TR segments ({sum(words_per_tr)} total words).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SNyaleHndF6"
      },
      "outputs": [],
      "source": [
        "# Extract contextualized embeddings from BERT\n",
        "# This step computes word-level activation vectors for each layer (13 total).\n",
        "# Note: this may take several minutes.\n",
        "\n",
        "%%time\n",
        "layer_acts_bert = get_layer_activations(model,\n",
        "                      tokenizer,\n",
        "                      hp_sentences)\n",
        "layer_acts_bert = np.concatenate(layer_acts_bert)\n",
        "print(f\"‚úÖ Extracted embeddings with shape: {layer_acts_bert.shape} (words √ó layers √ó dims)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dwf8l7RKnhQj"
      },
      "outputs": [],
      "source": [
        "# Average word embeddings within each TR\n",
        "# Each TR covers ~4 words ‚Üí average to obtain TR-level representations.\n",
        "tr_embeddings_bert = get_tr_embeddings(layer_acts_bert, words_per_tr)\n",
        "print(f\"‚úÖ Created TR embeddings: {tr_embeddings_bert.shape} (TRs √ó layers √ó dims)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Compare Model and Brain Representations (RSA)\n",
        "\n",
        "Now that we have activation vectors for each of the 1,295 TRs (one per fMRI scan) and each layer of BERT,  \n",
        "we can compute **Representational Dissimilarity Matrices (RDMs)** for both the model and the brain.\n",
        "\n",
        "- **Model RDMs:** quantify how similar or dissimilar the TR embeddings are to each other, separately for each of the 13 layers.  \n",
        "- **Brain RDM:** captures pairwise dissimilarities between voxel activation patterns across TRs.  \n",
        "- **RSA (Representational Similarity Analysis):** correlates these RDMs layer by layer to measure how well each model layer matches the brain‚Äôs representational geometry.\n",
        "\n",
        "**Prediction:** If deeper and more contextualized layers align better with the brain than the earlier layers, we should expect model RDMs of deeper layersm to correlate better with brain RDM.\n"
      ],
      "metadata": {
        "id": "zVuKhtTOUq3N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHhYtD04njMr"
      },
      "outputs": [],
      "source": [
        "# Compute model and brain RDMs\n",
        "# Create one RDM per BERT layer (13 total)\n",
        "RDMs_bert = [vector_distance_matrix(tr_embeddings_bert[:,layer,:],\n",
        "                                   metric=\"cosine\")\n",
        "             for layer in range(tr_embeddings_bert.shape[1])]\n",
        "\n",
        "# Create brain RDM from voxel activity patterns\n",
        "RDM_brain = vector_distance_matrix(brain_responses['voxel_signals'],\n",
        "                                   metric=\"cosine\")\n",
        "\n",
        "print(f\"‚úÖ Model RDM shape (example layer): {RDMs_bert[0].shape}\")\n",
        "print(f\"‚úÖ Brain RDM shape: {RDM_brain.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeEGzaHknmrb"
      },
      "outputs": [],
      "source": [
        "# Compute RSA: correlation between model and brain RDMs\n",
        "# use Pearson‚Äôs r to quantify similarity between RDM structures\n",
        "rsa_scores_bert = [compute_rsa_score(RDM_brain,\n",
        "                                     RDMs_bert[layer],\n",
        "                                     score=\"pearsonr\")\n",
        "                   for layer in range(len(RDMs_bert))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRnrxyUmnohb"
      },
      "source": [
        "As we see below, the correlation values themselves are quite low (they might get a bit higher if you provide the model with more context text). But we do observe the expected qualitative pattern: the higher layers with more contextualized embeddings score up to twice as high in representational similarity compared to lower layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BZ_SMrSno_M"
      },
      "outputs": [],
      "source": [
        "# Plot model‚Äìbrain representational similarity across layers\n",
        "\n",
        "plt.plot(rsa_scores_bert)\n",
        "plt.xlabel('model layer')\n",
        "plt.xticks(range(len(rsa_scores_bert)))\n",
        "plt.ylabel('similarity score (pearson\\'s r)')\n",
        "plt.title('Model-brain similarity over model layers')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Reflection\n",
        "Read the figure carefully and reflect on:\n",
        "- Which layers in BERT correspond better to human brain activity? Why?  \n",
        "- How might the pattern change if we used a different brain region (e.g., motor cortex)?  \n",
        "- What could explain why overall correlation scores remain small even when the trend is clear?\n"
      ],
      "metadata": {
        "id": "GRnmoGxxXmTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interpretation & Take-away\n",
        "- From the figure, we see that **deeper BERT layers show higher similarity to brain activity**, with the strongest correspondence around layers 9‚Äì11.  \n",
        "  - This suggests that **later layers capture more abstract and contextualized information**, similar to how higher-level language areas in the brain ‚Äî such as the **left anterior temporal lobe (LATL)** ‚Äî integrate words into coherent meanings.  \n",
        "- If we examined a **different brain region**, such as the motor cortex, we would likely see a **different pattern** or even no meaningful increase across layers, since that area is not primarily involved in language comprehension. (_Try it with Exercise 3!_)\n",
        "- Finally, the **low overall correlation values** reflect both the complexity and noise in fMRI data, as well as the fundamental differences between neural and model representations.  \n",
        "  - What matters here is not the absolute value, but the **relative trend** ‚Äî the gradual alignment between deeper model layers and semantic processing in the human brain.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OC9Xg-fiYIO3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gFYFAYwTAzy"
      },
      "source": [
        "## Exercise 3: Model‚ÄìBrain Alignment and Control Analyses\n",
        "\n",
        "In this final part, you‚Äôve examined how BERT‚Äôs layer-wise embeddings relate to human brain activity recorded during story reading. The RSA results you plotted showed that middle-to-higher model layers often align more closely with fMRI activity in language-related regions, such as the left anterior temporal lobe (LATL).\n",
        "\n",
        "Now it‚Äôs time to think critically about what this means ‚Äî and how we can verify that this relationship truly reflects **linguistic processing**, rather than coincidental patterns or noise.\n",
        "\n",
        "### 1. Evaluate and Extend the Analysis\n",
        "\n",
        "At first glance, the increasing similarity between deeper model layers and brain responses seems to support the idea that contextualized representations in language models mirror contextualized meaning in the brain. But how confident can we be in that interpretation?\n",
        "\n",
        "To validate this claim, let‚Äôs introduce a control or baseline comparison ‚Äî a scenario where you would not expect such alignment.\n",
        "Choose one of the following controls (or try both):\n",
        "** Option 1: Control analysis using a different brain region\n",
        "Use a non-language region as a control (e.g. a visual or motor area) and recompute the RSA scores.\n",
        "** Option 2: Compute a null baseline by Shuffling the brain RDM before RSA to test whether the observed correlations could arise by chance.\n",
        "\n",
        "### 2. Compare and reflect\n",
        "- Visualize your original and control RSA results on the same plot\n",
        "- Take a few minutes to interpret the comparison:\n",
        "  - Do deeper BERT layers still show stronger correlations in the control condition?\n",
        "  - How much lower are the RSA scores for the shuffled or non-language region?\n",
        "  - What does this tell us about the specificity of the model‚Äìbrain alignment?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab Report: Modeling Brain Activity with Your Choice of Language Models\n",
        "\n",
        "For the lab report, you will work in a group and extend the analysis from **Exercise 3** by comparing **language model representations** to **human fMRI responses** recorded during story reading, use your own choice of language models.  \n",
        "\n",
        "Your goal is to test whether (large) language models capture aspects of the neural representations that support human language comprehension.\n",
        "\n",
        "### 1. Choose a language model and extract model representations\n",
        "Select a language model - for example\n",
        "- a state-of-the-art large language model\n",
        "- a model with an architecture inspired by human cognition, or\n",
        "- a variant of BERT that has been modified to operate causally (i.e., processes text only from left to right).\n",
        "\n",
        "Briefly motivate your choice:\n",
        "  - What makes this model interesting or relevant for studying human language processing?  \n",
        "  - Is it more interpretable, more biologically plausible, or more powerful than models we‚Äôve seen before?\n",
        "\n",
        "_To simplify the analysis, you can choose one of the hiddern layers to compute your model embeddings._\n",
        "\n",
        "### 3. Compare Model and Brain Representations\n",
        "Compute **representational similarity** between your chosen model‚Äôs embeddings and fMRI data.  \n",
        "Use the RSA approach you implemented before, comparing model RDMs to brain RDMs.\n",
        "\n",
        "Extend this analysis from a single subject to the **group level**:\n",
        "- Compute RSA for each of the 8 available participants from the open dataset (http://www.cs.cmu.edu/~fmri/plosone/files/).  \n",
        "- Combine results across participants (e.g., by averaging similarity scores).\n",
        "\n",
        "**NB:** if you do not know how to run a group RSA analysis, see the method note below.\n",
        "\n",
        "### 4. Compute a Noise Ceiling\n",
        "Estimate a **noise ceiling** to understand the maximum achievable alignment given the noise in fMRI data.  \n",
        "The noise ceiling represents the upper bound on how well any model could predict brain responses, given inter-subject variability.\n",
        "\n",
        "Explain briefly:\n",
        "- How you estimated the noise ceiling (e.g., by measuring similarity between participants‚Äô brain RDMs).  \n",
        "- Why it is important for interpreting your results.  \n",
        "- How your model‚Äôs alignment compares to this upper bound.\n",
        "\n",
        "**NB:** if you do not know how to compute a noise ceiling, see the method note below.\n",
        "\n",
        "\n",
        "### 5. Visualize and Interpret\n",
        "Create clear, informative figures that illustrate your main findings, such as:\n",
        "- RSA curves for each model layer (if you choose to use all layers).\n",
        "- Group-level similarity scores  \n",
        "- A reference line showing the noise ceiling.\n",
        "- Comparison between brain regions.  \n",
        "\n",
        "**NB:** Normally, you will also perform some statistical testing to facilitate your interpretation of results. However, you can omit it here in this lab report, because (1) the total number of participants is still relatively small; (2) it is not a stats course after all ;)\n",
        "\n",
        "Then, interpret your findings in context:\n",
        "- Does your model outperform BERT or other baselines?  \n",
        "- How close do your results come to the noise ceiling?  \n",
        "- What might your findings suggest about how this model represents meaning?\n",
        "\n",
        "\n",
        "### 6. Write Your Lab Report\n",
        "Work together to produce a concise lab report that summarizes your analysis and findings.  \n",
        "Your report should include:\n",
        "\n",
        "1. **Introduction** ‚Äì What is the goal of your analysis? Which model did you choose, and why?  \n",
        "2. **Methods** ‚Äì Briefly describe your data, model representations, and analysis steps. Include a link to the codes you used, and describe briefly how your code works.  \n",
        "3. **Results** ‚Äì Present your RSA plots, group results with noise ceiling, and summarize your key findings.  \n",
        "4. **Discussion** ‚Äì Interpret your results and mention possible limitations.  \n"
      ],
      "metadata": {
        "id": "zCM5T3z3coy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method Note: How to Run a Group-Level RSA Analysis  \n",
        "\n",
        "When we test model‚Äìbrain similarity for a single subject, we learn how *that individual‚Äôs* brain represents language.  \n",
        "However, to make claims about **human language processing in general**, we need to verify that the same pattern holds **across participants**.  \n",
        "\n",
        "\n",
        "**Why group analysis matters**\n",
        "- Individual brains differ in many aspects.  \n",
        "- Averaging across participants reduces noise and highlights consistent trends.  \n",
        "- Group analysis lets us apply **inferential statistics**, allowing for scientifically interpretable conclusions.  \n",
        "\n",
        "**Steps for group RSA**\n",
        "1. Compute RSA per subject\n",
        "   - Extract brain RDMs within the target ROI (e.g., LATL).  \n",
        "   - Compute model RDMs (from a specified layer, or across layers)  \n",
        "   - Correlate model RDM with the subject‚Äôs brain RDM.  \n",
        "\n",
        "2. Aggregate across subjects  \n",
        "   - compute the **mean** RSA across subjects.  \n",
        "\n",
        "3. Visualize group results  \n",
        "    - Use a boxplot or strip plot (each dot = one subject).\n",
        "    - Overlay the noise ceiling\n",
        "\n",
        "4. Optional: Statistical testing\n",
        "  - You can test whether your results are **statistically significant** using one of the following approaches:\n",
        "    - **One-sided t-test against 0**: Tests if model‚Äìbrain similarity is greater than chance.(\"Does this model layer capture any reliable structure present in the brain across subjects?\")\n",
        "    - **Paired t-test between ROIs**: Compares two brain regions (e.g., LATL vs. visual cortex). (‚ÄúIs similarity higher in language regions than control regions?‚Äù\n",
        "    - **Permutation test**: Compares against a shuffled or randomized baseline.(more robust than the one-side t-test against 0)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NVQcGpm-hMbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method Note: How to compute noise ceilings\n",
        "\n",
        "When comparing model representations with brain data, it‚Äôs important to know **how high a correlation we could ever expect** ‚Äî given the measurement noise and individual variability in fMRI.  \n",
        "This upper bound is called the **noise ceiling**.\n",
        "\n",
        "Without it, an RSA score (e.g., *r* = 0.02) can be misleading ‚Äî it may look small, but if the best possible correlation within the data is *r* = 0.05, the model actually explains about 40% of the explainable variance.  \n",
        "\n",
        "\n",
        "**What the Noise Ceiling Represents**\n",
        "- The noise ceiling estimates the **maximum achievable similarity** between any perfect model and the brain data.  \n",
        "- It reflects the **reliability** of the neural responses themselves ‚Äî how consistent representational geometries are across participants.  \n",
        "- A higher noise ceiling means participants share similar brain patterns; a lower one indicates more noise or subject variability.  \n",
        "\n",
        "\n",
        "**The Leave-One-Out (LOO) Approach**  \n",
        "\n",
        "This method estimates how similar each participant‚Äôs brain representational geometry is to that of the group.\n",
        "\n",
        "1. **Compute each subject‚Äôs brain RDM** (within your ROI).   \n",
        "\n",
        "2. **Leave one subject out** and average the RDMs of the remaining participants to form a *group-average RDM*.  \n",
        "\n",
        "3. **Correlate the held-out subject‚Äôs RDM** with the group-average RDM (using the same measure, e.g., Pearson‚Äôs *r*).  \n",
        "\n",
        "4. **Repeat for all subjects.**  \n",
        "   This gives one correlation per participant:  \n",
        "   *noise_ceiling[i] = corr(RDM_subject[i], mean(RDM_others))*  \n",
        "\n",
        "5. **Take the average** of these values as the estimated **noise ceiling** for your ROI.  \n",
        "\n",
        "\n",
        "**Interpreting the Noise Ceiling**  \n",
        "\n",
        "- The noise ceiling defines the **maximum representational similarity** that any model could reach for your data.  \n",
        "- If your model‚Äìbrain RSA approaches the noise ceiling, the model captures most of the reliable signal.  \n",
        "- If it falls far below, the model only explains a small fraction of the variance that is theoretically predictable.  \n",
        "\n",
        "You can visualize it as a **horizontal line* on your group RSA plot to show how close the model comes to the data‚Äôs reliability limit.  \n",
        "\n"
      ],
      "metadata": {
        "id": "JAjUsIQS94cv"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.7.6 64-bit ('3.7.6')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "e9865b8fc30c7210bbbe2c0b0464dbc9700000eff1f4e229588b0a8358506f7f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}