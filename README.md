# Tutorial: Using Language Models to Probe Cognitive and Neural Representations
_For the course Cognitive Modelling (2025-2026, DataScience & AI)_

_Last updated: 26 October 2025_



In this tutorial, we will explore how linguistic meaning is represented both in **computational models** and in the **human brain**.  

1. **Part 1 – Static Word Embeddings**  
   We begin with classic embedding models such as **GloVe**, visualizing how words like *king*, *queen*, *man*, and *woman* are organized in a continuous semantic space.
2. **Part 2 – Contextualized Word Embeddings**  
   We then move to **BERT**, a transformer-based model that represents words differently depending on their context. You will compare how the meaning of the word *mouse* changes across sentences and compute representational similarity between embeddings.

3. **Part 3 – Linking Models to the Brain**  
   Finally, you will relate language model output to **fMRI data** recorded while participants read a story. Using **Representational Similarity Analysis (RSA)**, you will test whether deeper, more contextualized model layers better align with brain activity in language areas compared to early layers.

Together, these exercises will show how language models can serve as computational tools for studying human cognitive and neural representations.


Dr. Xiaochen Zheng

With the help of chatGPT


Based on: Advanced Neural and Cognitive Modelling (https://clclab.github.io/ANCM/intro.html) by J.Zuidema & M. de Heer Kloots
